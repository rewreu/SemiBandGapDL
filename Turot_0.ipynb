{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# old code from online instructions at https://contact.citrine.io/blog/2015/3/3/machine-learning-mat-sci-1\n",
    "# doesn't work, seems that developer at material project delete the API for make all combinations from periodic_table\n",
    "# from pymatgen import MPRester, periodic_table\n",
    "# import itertools\n",
    "\n",
    "# API_KEY = 'IJeQxmCAHlrKGW4T' # You have to register with Materials Project to receive an API\n",
    "\n",
    "# # There are 103 elements in pymatgen's list, giving C(103, 2) = 5253 binary systems\n",
    "# allBinaries = itertools.combinations(periodic_table.Specie, 2) # Create list of all binary systems\n",
    "\n",
    "# with MPRester(API_KEY) as m:\n",
    "#     for system in allBinaries:\n",
    "#         results = m.get_data(system[0] + '-' + system[1], data_type='vasp') # Download DFT data for each binary system\n",
    "#         for material in results: # We will receive many compounds within each binary system\n",
    "#             if material['e_above_hull'] < 1e-6: # Check if this compound is thermodynamically stable\n",
    "#                 print(material['pretty_formula'] + ',' + str(material['band_gap'])) # Output band gap csv to the screen      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The data below comes from following database\n",
    "http://bg.imet-db.ru/api/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import urllib2  # the lib that handles the url stuff\n",
    "data = urllib2.urlopen(\"http://bg.imet-db.ru/api/?mode=system&format=json\") # it's a file like object and works just like a file\n",
    "for line in data: # files are iterable\n",
    "    Temp=eval(line)\n",
    "    #print line\n",
    "# typo \")-Cr-Li-Te\"\n",
    "Temp[0]=\"Cr-Li-Te\"\n",
    "# typo \"0-Ga-Sn-Zn\"\n",
    "Temp[1]=\"Ga-Sn-Zn\"\n",
    "# typo 'Mn-S1-Sb-Sm'\n",
    "Temp[1772]=\"Mn-S-Sb-Sm\"\n",
    "# this one is not in material project\n",
    "Temp.remove(\"As-Pb-S-Sb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Somedata they are connected with ',' instead of '-', below is the code to replace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i,elem in enumerate(Temp):\n",
    "    if ',' in elem: \n",
    "#        print elem\n",
    "#        print elem.replace(',','-')\n",
    "        Temp[i]=elem.replace(',','-')\n",
    "# another mistake in the database to fix\n",
    "    if elem=='Bi-e-K-Rb': Temp[i]='Bi-Er-K-Rb' #\n",
    "    if elem=='Cu-Se-Si-Zm': Temp[i]='Cu-Se-Si-Sm' #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the index data to pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from six.moves import cPickle as pickle\n",
    "pickle_file = 'SemicoductorSystems.pickle'\n",
    "\n",
    "try:\n",
    "  f = open(pickle_file, 'wb')\n",
    "  save = {\n",
    "    'Namesys':Temp,\n",
    "    }\n",
    "  pickle.dump(save, f, pickle.HIGHEST_PROTOCOL)\n",
    "  f.close()\n",
    "except Exception as e:\n",
    "  print('Unable to save data to', pickle_file, ':', e)\n",
    "  raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the index data from pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from six.moves import cPickle as pickle\n",
    "pickle_file = 'SemicoductorSystems.pickle'\n",
    "with open(pickle_file, 'rb') as f:\n",
    "  save = pickle.load(f)\n",
    "  Temp = save['Namesys']\n",
    "  del save  # hint to help gc free up memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dowload the data and Save to pickle file \n",
    "Now lets download some data from material project data base at \n",
    "https://materialsproject.org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from pymatgen import MPRester, periodic_table\n",
    "import itertools\n",
    "\n",
    "API_KEY = 'IJeQxmCAHlrKGW4T' # You have to register with Materials Project to receive an API\n",
    "#partial=10\n",
    "partial=len(Temp)\n",
    "breakpoint=1162\n",
    "Temp2=Temp[breakpoint:partial]\n",
    "#database=[]\n",
    "\n",
    "import time\n",
    "start=time.time()\n",
    "\n",
    "with MPRester(API_KEY) as m:\n",
    "#    results = m.get_data(Temp[190], data_type='vasp')\n",
    "    for system in Temp2:\n",
    "        results = m.get_data(system, data_type='vasp') # Download DFT data for each binary system\n",
    "        if results !=[]:\n",
    "          for material in results: # We will receive many compounds within each binary system\n",
    "            if (material['band_gap']>0) and (material['e_above_hull'] < 1e-6): # Check if this compound is thermodynamically stable\n",
    "                #print(material['pretty_formula'] + ',' + str(material['band_gap'])) # Output band gap csv to the screen \n",
    "                database.append(material)\n",
    "timecost=time.time()-start\n",
    "#write to SemiDataBase pickle file\n",
    "from six.moves import cPickle as pickle\n",
    "pickle_file = 'SemiDataBase.pickle'\n",
    "\n",
    "try:\n",
    "  f = open(pickle_file, 'wb')\n",
    "  save = {\n",
    "    'database':database,\n",
    "    }\n",
    "  pickle.dump(save, f, pickle.HIGHEST_PROTOCOL)\n",
    "  f.close()\n",
    "except Exception as e:\n",
    "  print('Unable to save data to', pickle_file, ':', e)\n",
    "  raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the saved Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from six.moves import cPickle as pickle\n",
    "pickle_file = 'SemiDataBase.pickle'\n",
    "with open(pickle_file, 'rb') as f:\n",
    "  save = pickle.load(f)\n",
    "  database= save['database']\n",
    "  del save  # hint to help gc free up memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# a quick look at 11th item of data\n",
    "for key in database[11]:\n",
    "    print key,\">>>>>>>>>>>>>>>>>>>>>\",database[11][key]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The code below gives an intuitive look at the elements distribution in periodic table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elements appear in the dataset\n",
      "##############################################################################################\n",
      "#    H   **   **   **   **   **   **   **   **   **   **   **   **   **   **   **   **   **  #\n",
      "#   Li   Be   **   **   **   **   **   **   **   **   **   **    B    C    N    O    F   **  #\n",
      "#   Na   Mg   **   **   **   **   **   **   **   **   **   **   Al   Si    P    S   Cl   **  #\n",
      "#    K   Ca   Sc   Ti    V   Cr   Mn   Fe   Co   Ni   Cu   Zn   Ga   Ge   As   Se   Br   **  #\n",
      "#   Rb   Sr    Y   Zr   Nb   Mo   Tc   Ru   Rh   Pd   Ag   Cd   In   Sn   Sb   Te    I   **  #\n",
      "#   Cs   Ba   **   Hf   Ta    W   Re   Os   Ir   Pt   Au   Hg   Tl   Pb   Bi   **   **   **  #\n",
      "#   **   **   **   **   **   **   **   **   **   **   **   **   **   **   **   **   **   **  #\n",
      "#   **   **   La   Ce   Pr   Nd   **   Sm   Eu   Gd   Tb   Dy   Ho   Er   Tm   Yb   Lu   **  #\n",
      "#   **   **   **   Th   **    U   **   **   **   **   **   **   **   **   **   **   **   **  #\n",
      "##############################################################################################\n",
      "Count of the elements appearance\n",
      "##############################################################################################\n",
      "#   38    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0  #\n",
      "#   70    5    0    0    0    0    0    0    0    0    0    0  103   12   39  523   33    0  #\n",
      "#   90   38    0    0    0    0    0    0    0    0    0    0   28   48  168  343   43    0  #\n",
      "#  175   56   14   34   40   14   25   15   22   17   98   48   64   73  101  295   42    0  #\n",
      "#   87   48   12   10   45   32    2   13    4   15   61   68   68   67  105  148   71    0  #\n",
      "#   99  107    0    6   46    5    5    3    7   11    7   37   79   50  126    0    0    0  #\n",
      "#    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0  #\n",
      "#    0    0   39   14   20   16    0   25   11   12   12   13   10    9    3    8    6    0  #\n",
      "#    0    0    0   13    0    6    0    0    0    0    0    0    0    0    0    0    0    0  #\n",
      "##############################################################################################\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pymatgen import MPRester, periodic_table\n",
    "from pymatgen import Composition, Element\n",
    "import itertools\n",
    "# list1=[1,2,3]\n",
    "# 4 in list1\n",
    "list2=[]\n",
    "# for ele in database[1][\"elements\"]:\n",
    "#     print ele\n",
    "#     print type(ele)\n",
    "#     list2.append(ele)\n",
    "\n",
    "# Creates a list containing 5 lists, each of 8 items, all set to 0\n",
    "\n",
    "# creat an matrix for periorod table\n",
    "\n",
    "w, h = 18, 9;\n",
    "Matrix = [['**' for x in range(w)] for y in range(h)] \n",
    "Matrix2 = [[0 for x in range(w)] for y in range(h)] \n",
    "\n",
    "for data in database:\n",
    "    for elem in data[\"elements\"]:\n",
    "#         elem.row()\n",
    "        if elem not in list2:\n",
    "            list2.append(elem)\n",
    "            Matrix[Element(elem).row-1][Element(elem).group-1]=elem\n",
    "for data in database:\n",
    "    for elem in data[\"elements\"]:          \n",
    "            Matrix2[Element(elem).row-1][Element(elem).group-1]=Matrix2[Element(elem).row-1][Element(elem).group-1]+1\n",
    "\n",
    "print \"elements appear in the dataset\"\n",
    "print \"#\"*94\n",
    "for row in range(h): \n",
    "    print \"#\",\n",
    "    for col in range(w):\n",
    "        print '%4s' % Matrix[row][col],\n",
    "    print \" #\",\n",
    "    print\n",
    "print \"#\"*94\n",
    "print \"Count of the elements appearance\"\n",
    "print \"#\"*94\n",
    "for row in range(h): \n",
    "    print \"#\",\n",
    "    for col in range(w):\n",
    "        print '%4d' % Matrix2[row][col],\n",
    "    print \" #\",\n",
    "    print\n",
    "print \"#\"*94"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## manipulate the data(make into traning data set format)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This step will make three data sets, PTFeatures will be vectors with (9x18) dimensions represents the elements in periodic table. bandgaps will contains all the bandgap info."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data dimensions is (1530, 9, 18)\n",
      "label dimensions is (1530,)\n"
     ]
    }
   ],
   "source": [
    "materials = []\n",
    "bandgaps = []\n",
    "PTFeatures = []\n",
    "\n",
    "for item in database:\n",
    "    materials.append(item[\"full_formula\"])\n",
    "    bandgaps.append(item[\"band_gap\"])\n",
    "\n",
    "def PeriodicTableVectorize(composition):\n",
    "       vector = zeros((9,18)) # size of periodic table\n",
    "       for element in composition:\n",
    "               fraction = composition.get_atomic_fraction(element)\n",
    "               vector[element.row-1,element.group-1] = fraction\n",
    "       return(vector)\n",
    "\n",
    "for item in materials:\n",
    "       material = Composition(item)\n",
    "       PTFeatures.append(PeriodicTableVectorize(material)) #create features from chemical formula\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "Fulldata=np.asarray(PTFeatures)\n",
    "print \"data dimensions is\", Fulldata.shape\n",
    "labels=np.asarray(bandgaps)\n",
    "print \"label dimensions is\", labels.shape\n",
    "# since the data it self is "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def randomize(dataset, labels):\n",
    "  permutation = np.random.permutation(labels.shape[0])\n",
    "  shuffled_dataset = dataset[permutation,:,:]\n",
    "  shuffled_labels = labels[permutation]\n",
    "  return shuffled_dataset, shuffled_labels\n",
    "# train_dataset, train_labels = randomize(train_dataset, train_labels)\n",
    "# test_dataset, test_labels = randomize(test_dataset, test_labels)\n",
    "# valid_dataset, valid_labels = randomize(valid_dataset, valid_labels)\n",
    "\n",
    "# shuffle  the data\n",
    "Fulldata,labels=randomize(Fulldata,labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save machine learning data to pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from six.moves import cPickle as pickle\n",
    "pickle_file = 'MachineLearningData.pickle'\n",
    "\n",
    "try:\n",
    "  f = open(pickle_file, 'wb')\n",
    "  save = {\n",
    "    'Fulldata':Fulldata,\n",
    "    'labels':labels,\n",
    "    }\n",
    "  pickle.dump(save, f, pickle.HIGHEST_PROTOCOL)\n",
    "  f.close()\n",
    "except Exception as e:\n",
    "  print('Unable to save data to', pickle_file, ':', e)\n",
    "  raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load machine learning data to pickle "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from six.moves import cPickle as pickle\n",
    "pickle_file = 'MachineLearningData.pickle'\n",
    "with open(pickle_file, 'rb') as f:\n",
    "  save = pickle.load(f)\n",
    "  Fulldata = save['Fulldata']\n",
    "  labels = save['labels']\n",
    "  del save  # hint to help gc free up memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Machine learning\n",
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The MAE of always guessing the average band gap is: 1.081 eV\n"
     ]
    }
   ],
   "source": [
    "baselineError = mean(abs(mean(train_labels) - train_labels))\n",
    "print(\"The MAE of always guessing the average band gap is: \" + str(round(baselineError, 3)) + \" eV\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The MAE of the linear ridge regression band gap model using the naive feature set is: 0.683 eV\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAhwAAAF5CAYAAADUL/MIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fe0ee965610>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Train linear ridge regression model using naive feature set\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn import linear_model, cross_validation, metrics, ensemble\n",
    "\n",
    "train_dataset=Fulldata[0:1300]\n",
    "train_labels=labels[0:1300]\n",
    "test_dataset=Fulldata[1300:1531]\n",
    "test_labels=labels[1300:1531]\n",
    "\n",
    "num_samples=train_dataset.shape[0]\n",
    "#this won't work if num_samples are too small, if num_samples is too small, all Y is 0( the first number type), so it has to be large engouh\n",
    "(samples, width, height) = train_dataset.shape\n",
    "X = np.reshape(train_dataset,(samples,width*height))[0:num_samples]\n",
    "Y = train_labels[0:num_samples]\n",
    "\n",
    "#alpha is a tuning parameter affecting how regression deals with collinear inputs\n",
    "linear = linear_model.Ridge(alpha = 0.6)  \n",
    "\n",
    "cv = cross_validation.ShuffleSplit(len(Y),n_iter=10, test_size=0.1, random_state=0)\n",
    "\n",
    "scores = cross_validation.cross_val_score(linear, X,Y, cv=cv, scoring='mean_absolute_error')\n",
    "\n",
    "print(\"The MAE of the linear ridge regression band gap model using the naive feature set is: \"+ str(round(abs(mean(scores)), 3)) + \" eV\")\n",
    "############# plot it##########################################\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "predicted = cross_val_predict(linear, X, Y, cv=10)\n",
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(Y, predicted)\n",
    "ax.plot([Y.min(), Y.max()], [Y.min(), Y.max()], 'k--', lw=2)\n",
    "ax.set_xlabel('Measured')\n",
    "ax.set_ylabel('Predicted')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural networks\n",
    "### without hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Training set', (1100, 162), (1100, 1))\n",
      "('Validation set', (200, 162), (200, 1))\n",
      "('Test set', (230, 162), (230, 1))\n"
     ]
    }
   ],
   "source": [
    "# image_size = 28\n",
    "num_labels = 1\n",
    "train_end=1100\n",
    "valid_end=1300\n",
    "train_dataset=Fulldata[0:train_end]\n",
    "train_labels=labels[0:train_end]\n",
    "valid_dataset=Fulldata[train_end:valid_end]\n",
    "valid_labels=labels[train_end:valid_end]\n",
    "test_dataset=Fulldata[valid_end:1531]\n",
    "test_labels=labels[valid_end:1531]\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "  dataset = dataset.reshape((-1, Fulldata.shape[1] * Fulldata.shape[2])).astype(np.float32)\n",
    "  labels = (labels[:,None]).astype(np.float32)\n",
    "  return dataset, labels\n",
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Loss at step 0: 1.883463\n",
      "Validation Loss: 1.584549\n",
      "Loss at step 10000: 0.636692\n",
      "Validation Loss: 0.687881\n",
      "Loss at step 20000: 0.631331\n",
      "Validation Loss: 0.686636\n",
      "Loss at step 30000: 0.629745\n",
      "Validation Loss: 0.683074\n",
      "Loss at step 40000: 0.629499\n",
      "Validation Loss: 0.682205\n",
      "Test Loss: 0.664910\n"
     ]
    }
   ],
   "source": [
    "# With gradient descent training, even this much data is prohibitive.\n",
    "# Subset the training data for faster turnaround.\n",
    "\n",
    "train_subset = train_end\n",
    "# label dimention is 1\n",
    "num_labels=1\n",
    "import tensorflow as tf\n",
    "from tensorflow import contrib\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  tf_train_dataset = tf.constant(train_dataset[:train_subset, :])\n",
    "  tf_train_labels = tf.constant(train_labels[:train_subset])\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_valid_labels = tf.constant(valid_labels)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  tf_test_labels = tf.constant(test_labels)\n",
    "\n",
    "  weights = tf.Variable(tf.truncated_normal([Fulldata.shape[1] * Fulldata.shape[2], num_labels]))\n",
    "  biases = tf.Variable(tf.zeros([num_labels]))\n",
    "\n",
    "  logits = tf.matmul(tf_train_dataset, weights) + biases\n",
    "  loss = tf.reduce_mean(tf.abs(logits-tf_train_labels))\n",
    "\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "  train_prediction = logits  \n",
    "\n",
    "  valid_prediction = tf.matmul(tf_valid_dataset, weights) + biases\n",
    "  valid_loss = tf.reduce_mean(tf.abs(valid_prediction-tf_valid_labels))\n",
    "    \n",
    "  test_prediction = tf.matmul(tf_test_dataset, weights) + biases\n",
    "  test_loss = tf.reduce_mean(tf.abs(test_prediction-tf_test_labels))\n",
    "#########################################################################################3\n",
    "num_steps = 50000\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print('Initialized')\n",
    "  for step in range(num_steps):\n",
    "    _, l, predictions = session.run([optimizer, loss, train_prediction])\n",
    "    if (step % 10000 == 0):\n",
    "      print('Loss at step %d: %f' % (step, l))\n",
    "      print('Validation Loss: %f' % valid_loss.eval())\n",
    "  print('Test Loss: %f' %  test_loss.eval())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## with one hidden layer, train with batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Loss at step 0: 9.913927\n",
      "Validation Loss: 28.716265\n",
      "Loss at step 1000: 0.553740\n",
      "Validation Loss: 0.936711\n",
      "Loss at step 2000: 0.622823\n",
      "Validation Loss: 0.750750\n",
      "Loss at step 3000: 0.440426\n",
      "Validation Loss: 0.861358\n",
      "Loss at step 4000: 0.458674\n",
      "Validation Loss: 0.795093\n",
      "Loss at step 5000: 0.436431\n",
      "Validation Loss: 0.765197\n",
      "Loss at step 6000: 0.478887\n",
      "Validation Loss: 0.790238\n",
      "Loss at step 7000: 0.420878\n",
      "Validation Loss: 0.786454\n",
      "Loss at step 8000: 0.320174\n",
      "Validation Loss: 0.656050\n",
      "Loss at step 9000: 0.290202\n",
      "Validation Loss: 0.717809\n",
      "Test Loss: 0.690147\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "num_hidden_nodes = 1024\n",
    "\n",
    "train_subset = train_end\n",
    "batch_size = train_subset/10\n",
    "num_hidden_nodes = 1024\n",
    "# label dimention is 1\n",
    "num_labels=1\n",
    "import tensorflow as tf\n",
    "from tensorflow import contrib\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,shape=(batch_size, 162))  \n",
    "  #tf_train_dataset = tf.constant(train_dataset[:batch_size, :])\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size,1))\n",
    "  #tf_train_labels = tf.constant(train_labels[:batch_size])\n",
    "\n",
    "  weights_1 = tf.Variable(tf.truncated_normal([Fulldata.shape[1] * Fulldata.shape[2], num_hidden_nodes]))\n",
    "  weights_2 =  tf.Variable(tf.truncated_normal([num_hidden_nodes, num_labels]))\n",
    "  biases_1 = tf.Variable(tf.zeros([num_hidden_nodes]))\n",
    "  biases_2 = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  layer_1 = tf.nn.relu(tf.matmul(tf_train_dataset, weights_1) + biases_1)\n",
    "  logits = tf.matmul(layer_1, weights_2) + biases_2\n",
    "  loss = tf.reduce_mean(tf.abs(logits-tf_train_labels))\n",
    "\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "  train_prediction = logits  \n",
    "\n",
    "#########################################################################################    \n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_valid_labels = tf.constant(valid_labels)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  tf_test_labels = tf.constant(test_labels)\n",
    "\n",
    "  layer_1_valid = tf.nn.relu(tf.matmul(tf_valid_dataset, weights_1) + biases_1)\n",
    "  valid_prediction = tf.matmul(layer_1_valid, weights_2) + biases_2\n",
    "  valid_loss = tf.reduce_mean(tf.abs(valid_prediction-tf_valid_labels))\n",
    "    \n",
    "  layer_1_test = tf.nn.relu(tf.matmul(tf_test_dataset, weights_1) + biases_1)\n",
    "  test_prediction = tf.matmul(layer_1_test, weights_2) + biases_2\n",
    "  test_loss = tf.reduce_mean(tf.abs(test_prediction-tf_test_labels))\n",
    "  #########################################################################################3\n",
    "num_steps = 10000\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print('Initialized')\n",
    "  for step in range(num_steps):\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    #_, l, predictions = session.run([optimizer, loss, train_prediction])\n",
    "    _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 1000 == 0):\n",
    "      print('Loss at step %d: %f' % (step, l))\n",
    "      print('Validation Loss: %f' % valid_loss.eval())\n",
    "  print('Test Loss: %f' %  test_loss.eval())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add regulation to previous one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Loss at step 0: 75.267990\n",
      "Validation Loss: 17.111399\n",
      "Loss at step 10000: 0.614537\n",
      "Validation Loss: 0.646773\n",
      "Loss at step 20000: 0.731811\n",
      "Validation Loss: 0.788381\n",
      "Loss at step 30000: 0.672801\n",
      "Validation Loss: 0.639590\n",
      "Loss at step 40000: 0.649371\n",
      "Validation Loss: 0.679590\n",
      "Loss at step 50000: 0.613999\n",
      "Validation Loss: 0.703059\n",
      "Loss at step 60000: 0.612504\n",
      "Validation Loss: 0.822633\n",
      "Loss at step 70000: 0.581319\n",
      "Validation Loss: 0.941529\n",
      "Loss at step 80000: 0.696750\n",
      "Validation Loss: 0.687981\n",
      "Loss at step 90000: 0.505735\n",
      "Validation Loss: 0.599858\n",
      "Test Loss: 0.608422\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "num_hidden_nodes = 1024\n",
    "# so far 1e-3 yields best result 0.599\n",
    "beta=1e-3\n",
    "\n",
    "train_subset = train_end\n",
    "batch_size = train_subset/10\n",
    "num_hidden_nodes = 1024\n",
    "# label dimention is 1\n",
    "num_labels=1\n",
    "import tensorflow as tf\n",
    "from tensorflow import contrib\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,shape=(batch_size, 162))  \n",
    "  #tf_train_dataset = tf.constant(train_dataset[:batch_size, :])\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size,1))\n",
    "  #tf_train_labels = tf.constant(train_labels[:batch_size])\n",
    "\n",
    "  weights_1 = tf.Variable(tf.truncated_normal([Fulldata.shape[1] * Fulldata.shape[2], num_hidden_nodes]))\n",
    "  weights_2 =  tf.Variable(tf.truncated_normal([num_hidden_nodes, num_labels]))\n",
    "  biases_1 = tf.Variable(tf.zeros([num_hidden_nodes]))\n",
    "  biases_2 = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  layer_1 = tf.nn.relu(tf.matmul(tf_train_dataset, weights_1) + biases_1)\n",
    "  logits = tf.matmul(layer_1, weights_2) + biases_2\n",
    "  loss = tf.reduce_mean(tf.abs(logits-tf_train_labels))+beta*(tf.nn.l2_loss(weights_1)+tf.nn.l2_loss(weights_2))\n",
    "\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "  \n",
    "  train_prediction = logits  \n",
    "\n",
    "#########################################################################################    \n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_valid_labels = tf.constant(valid_labels)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  tf_test_labels = tf.constant(test_labels)\n",
    "\n",
    "  layer_1_valid = tf.nn.relu(tf.matmul(tf_valid_dataset, weights_1) + biases_1)\n",
    "  valid_prediction = tf.matmul(layer_1_valid, weights_2) + biases_2\n",
    "  valid_loss = tf.reduce_mean(tf.abs(valid_prediction-tf_valid_labels))\n",
    "    \n",
    "  layer_1_test = tf.nn.relu(tf.matmul(tf_test_dataset, weights_1) + biases_1)\n",
    "  test_prediction = tf.matmul(layer_1_test, weights_2) + biases_2\n",
    "  test_loss = tf.reduce_mean(tf.abs(test_prediction-tf_test_labels))\n",
    "  #########################################################################################3\n",
    "num_steps = 100000\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print('Initialized')\n",
    "  for step in range(num_steps):\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    #_, l, predictions = session.run([optimizer, loss, train_prediction])\n",
    "    _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 10000 == 0):\n",
    "      print('Loss at step %d: %f' % (step, l))\n",
    "      print('Validation Loss: %f' % valid_loss.eval())\n",
    "  print('Test Loss: %f' %  test_loss.eval())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropout added to previous test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Loss at step 0: 75.242477\n",
      "Validation Loss: 70.626396\n",
      "Loss at step 1000: 24.033871\n",
      "Validation Loss: 0.669397\n",
      "Loss at step 2000: 9.339220\n",
      "Validation Loss: 0.742051\n",
      "Loss at step 3000: 3.788256\n",
      "Validation Loss: 0.820103\n",
      "Loss at step 4000: 1.759256\n",
      "Validation Loss: 0.614750\n",
      "Loss at step 5000: 0.956293\n",
      "Validation Loss: 0.597205\n",
      "Loss at step 6000: 0.883665\n",
      "Validation Loss: 0.632919\n",
      "Loss at step 7000: 0.872911\n",
      "Validation Loss: 0.643003\n",
      "Loss at step 8000: 0.791690\n",
      "Validation Loss: 0.668034\n",
      "Loss at step 9000: 0.552466\n",
      "Validation Loss: 0.662916\n",
      "Test Loss: 0.615167\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "num_hidden_nodes = 1024\n",
    "# so far 1e-3 yields best result 0.599\n",
    "beta=1e-3\n",
    "\n",
    "train_subset = train_end\n",
    "batch_size = train_subset/10\n",
    "num_hidden_nodes = 1024\n",
    "# label dimention is 1\n",
    "num_labels=1\n",
    "import tensorflow as tf\n",
    "from tensorflow import contrib\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,shape=(batch_size, 162))  \n",
    "  #tf_train_dataset = tf.constant(train_dataset[:batch_size, :])\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size,1))\n",
    "  #tf_train_labels = tf.constant(train_labels[:batch_size])\n",
    "\n",
    "  weights_1 = tf.Variable(tf.truncated_normal([Fulldata.shape[1] * Fulldata.shape[2], num_hidden_nodes]))\n",
    "  weights_2 =  tf.Variable(tf.truncated_normal([num_hidden_nodes, num_labels]))\n",
    "  biases_1 = tf.Variable(tf.zeros([num_hidden_nodes]))\n",
    "  biases_2 = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  layer_1 = tf.nn.dropout(tf.nn.relu(tf.matmul(tf_train_dataset, weights_1) + biases_1),keep_prob=0.8)\n",
    "  logits = tf.matmul(layer_1, weights_2) + biases_2\n",
    "  loss = tf.reduce_mean(tf.abs(logits-tf_train_labels))+beta*(tf.nn.l2_loss(weights_1)+tf.nn.l2_loss(weights_2))\n",
    "\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "#########################################################################################\n",
    "  layer_1_train= tf.nn.relu(tf.matmul(tf_train_dataset, weights_1) + biases_1)\n",
    "  train_prediction = tf.matmul(layer_1_train, weights_2) + biases_2  \n",
    "\n",
    "#########################################################################################    \n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_valid_labels = tf.constant(valid_labels)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  tf_test_labels = tf.constant(test_labels)\n",
    "\n",
    "  layer_1_valid = tf.nn.relu(tf.matmul(tf_valid_dataset, weights_1) + biases_1)\n",
    "  valid_prediction = tf.matmul(layer_1_valid, weights_2) + biases_2\n",
    "  valid_loss = tf.reduce_mean(tf.abs(valid_prediction-tf_valid_labels))\n",
    "    \n",
    "  layer_1_test = tf.nn.relu(tf.matmul(tf_test_dataset, weights_1) + biases_1)\n",
    "  test_prediction = tf.matmul(layer_1_test, weights_2) + biases_2\n",
    "  test_loss = tf.reduce_mean(tf.abs(test_prediction-tf_test_labels))\n",
    "  #########################################################################################3\n",
    "num_steps = 10000\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print('Initialized')\n",
    "  for step in range(num_steps):\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    #_, l, predictions = session.run([optimizer, loss, train_prediction])\n",
    "    _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 1000 == 0):\n",
    "      print('Loss at step %d: %f' % (step, l))\n",
    "      print('Validation Loss: %f' % valid_loss.eval())\n",
    "  print('Test Loss: %f' %  test_loss.eval())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Below try to use different initial value(stddev) for weight to see if accuracy can be improved,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Loss at step 0: 2.575462\n",
      "Validation Loss: 1.151463\n",
      "Loss at step 1000: 0.983694\n",
      "Validation Loss: 0.774037\n",
      "Loss at step 2000: 0.818250\n",
      "Validation Loss: 0.799166\n",
      "Loss at step 3000: 0.633846\n",
      "Validation Loss: 0.695941\n",
      "Loss at step 4000: 0.804100\n",
      "Validation Loss: 0.691766\n",
      "Loss at step 5000: 0.575336\n",
      "Validation Loss: 0.676020\n",
      "Loss at step 6000: 0.821887\n",
      "Validation Loss: 0.695752\n",
      "Loss at step 7000: 0.557278\n",
      "Validation Loss: 0.699531\n",
      "Loss at step 8000: 0.676409\n",
      "Validation Loss: 0.649440\n",
      "Loss at step 9000: 0.590448\n",
      "Validation Loss: 0.710368\n",
      "Test Loss: 0.653532\n"
     ]
    }
   ],
   "source": [
    "import math as math\n",
    "batch_size = 128\n",
    "num_hidden_nodes = 1024\n",
    "# so far 1e-3 yields best result 0.599\n",
    "beta=1e-3\n",
    "\n",
    "train_subset = train_end\n",
    "batch_size = train_subset/10\n",
    "num_hidden_nodes = 1024\n",
    "# label dimention is 1\n",
    "num_labels=1\n",
    "import tensorflow as tf\n",
    "from tensorflow import contrib\n",
    "graph = tf.Graph()\n",
    "dimentions=Fulldata.shape[1] * Fulldata.shape[2]\n",
    "with graph.as_default():\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,shape=(batch_size, 162))  \n",
    "  #tf_train_dataset = tf.constant(train_dataset[:batch_size, :])\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size,1))\n",
    "  #tf_train_labels = tf.constant(train_labels[:batch_size])\n",
    "\n",
    "  weights_1 = tf.Variable(tf.truncated_normal([dimentions, num_hidden_nodes],\n",
    "                                              stddev=math.sqrt(2.0/dimentions)))\n",
    "  weights_2 =  tf.Variable(tf.truncated_normal([num_hidden_nodes, num_labels],\n",
    "                                               stddev=math.sqrt(2.0/(num_hidden_nodes))))\n",
    "  biases_1 = tf.Variable(tf.zeros([num_hidden_nodes]))\n",
    "  biases_2 = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  layer_1 = tf.nn.dropout(tf.nn.relu(tf.matmul(tf_train_dataset, weights_1) + biases_1),keep_prob=0.8)\n",
    "  logits = tf.matmul(layer_1, weights_2) + biases_2\n",
    "  loss = tf.reduce_mean(tf.abs(logits-tf_train_labels))+beta*(tf.nn.l2_loss(weights_1)+tf.nn.l2_loss(weights_2))\n",
    "\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "#########################################################################################  \n",
    "  layer_1_train= tf.nn.relu(tf.matmul(tf_train_dataset, weights_1) + biases_1)\n",
    "  train_prediction = tf.matmul(layer_1_train, weights_2) + biases_2  \n",
    "\n",
    "\n",
    "#########################################################################################    \n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_valid_labels = tf.constant(valid_labels)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  tf_test_labels = tf.constant(test_labels)\n",
    "\n",
    "  layer_1_valid = tf.nn.relu(tf.matmul(tf_valid_dataset, weights_1) + biases_1)\n",
    "  valid_prediction = tf.matmul(layer_1_valid, weights_2) + biases_2\n",
    "  valid_loss = tf.reduce_mean(tf.abs(valid_prediction-tf_valid_labels))\n",
    "    \n",
    "  layer_1_test = tf.nn.relu(tf.matmul(tf_test_dataset, weights_1) + biases_1)\n",
    "  test_prediction = tf.matmul(layer_1_test, weights_2) + biases_2\n",
    "  test_loss = tf.reduce_mean(tf.abs(test_prediction-tf_test_labels))\n",
    "  #########################################################################################3\n",
    "num_steps = 10000\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print('Initialized')\n",
    "  for step in range(num_steps):\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    #_, l, predictions = session.run([optimizer, loss, train_prediction])\n",
    "    _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 1000 == 0):\n",
    "      print('Loss at step %d: %f' % (step, l))\n",
    "      print('Validation Loss: %f' % valid_loss.eval())\n",
    "  print('Test Loss: %f' %  test_loss.eval())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Loss at step 0: 3.046521\n",
      "Validation Loss: 1.232555\n",
      "Loss at step 1000: 1.095721\n",
      "Validation Loss: 0.630546\n"
     ]
    }
   ],
   "source": [
    "import math as math\n",
    "batch_size = 128\n",
    "num_hidden_nodes = 1024\n",
    "num_hidden_nodes2=512\n",
    "# so far 1e-3 yields best result 0.599\n",
    "beta=1e-3\n",
    "\n",
    "train_subset = train_end\n",
    "batch_size = train_subset/10\n",
    "num_hidden_nodes = 1024\n",
    "# label dimention is 1\n",
    "num_labels=1\n",
    "import tensorflow as tf\n",
    "from tensorflow import contrib\n",
    "graph = tf.Graph()\n",
    "dimentions=Fulldata.shape[1] * Fulldata.shape[2]\n",
    "with graph.as_default():\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,shape=(batch_size, 162))  \n",
    "  #tf_train_dataset = tf.constant(train_dataset[:batch_size, :])\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size,1))\n",
    "  #tf_train_labels = tf.constant(train_labels[:batch_size])\n",
    "\n",
    "  weights_1 = tf.Variable(tf.truncated_normal([dimentions, num_hidden_nodes],\n",
    "                                              stddev=math.sqrt(2.0/dimentions)))\n",
    "  biases_1 = tf.Variable(tf.zeros([num_hidden_nodes]))\n",
    "  weights_2=tf.Variable(tf.truncated_normal([num_hidden_nodes, num_hidden_nodes2],\n",
    "                                             stddev=math.sqrt(2.0/(num_hidden_nodes))))\n",
    "  biases_2=tf.Variable(tf.zeros([num_hidden_nodes2]))\n",
    "  \n",
    "  weights_o =  tf.Variable(tf.truncated_normal([num_hidden_nodes2, num_labels],\n",
    "                                               stddev=math.sqrt(2.0/(num_hidden_nodes))))  \n",
    "  biases_o = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  layer_1 = tf.nn.dropout(tf.nn.relu(tf.matmul(tf_train_dataset, weights_1) + biases_1),keep_prob=0.8)\n",
    "  layer_2=tf.nn.dropout(tf.nn.relu(tf.matmul(layer_1,weights_2)+biases_2),0.75)\n",
    "  \n",
    "  logits = tf.matmul(layer_2, weights_o) + biases_o\n",
    "  \n",
    "  loss = tf.reduce_mean(tf.abs(logits-tf_train_labels))+beta*(tf.nn.l2_loss(weights_1)+tf.nn.l2_loss(weights_o)+tf.nn.l2_loss(weights_2))\n",
    "\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "#########################################################################################  \n",
    "  layer_1_train=tf.nn.relu(tf.matmul(tf_train_dataset,weights_1)+biases_1)\n",
    "  layer_2_train=tf.nn.relu(tf.matmul(layer_1_train,weights_2)+biases_2)\n",
    "  train_prediction = tf.matmul(layer_2_train, weights_o) + biases_o  \n",
    "\n",
    "#########################################################################################    \n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_valid_labels = tf.constant(valid_labels)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  tf_test_labels = tf.constant(test_labels)\n",
    "\n",
    "  layer_1_valid = tf.nn.relu(tf.matmul(tf_valid_dataset, weights_1) + biases_1)\n",
    "  layer_2_valid = tf.nn.relu(tf.matmul(layer_1_valid, weights_2) + biases_2)  \n",
    "  valid_prediction = tf.matmul(layer_2_valid, weights_o) + biases_o\n",
    "  valid_loss = tf.reduce_mean(tf.abs(valid_prediction-tf_valid_labels))\n",
    "    \n",
    "  layer_1_test = tf.nn.relu(tf.matmul(tf_test_dataset, weights_1) + biases_1)\n",
    "  layer_2_test = tf.nn.relu(tf.matmul(layer_1_test, weights_2) + biases_2)\n",
    "  test_prediction = tf.matmul(layer_2_test, weights_o) + biases_o\n",
    "  test_loss = tf.reduce_mean(tf.abs(test_prediction-tf_test_labels))\n",
    "  #########################################################################################3\n",
    "num_steps = 10000\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print('Initialized')\n",
    "  for step in range(num_steps):\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    #_, l, predictions = session.run([optimizer, loss, train_prediction])\n",
    "    _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 1000 == 0):\n",
    "      print('Loss at step %d: %f' % (step, l))\n",
    "      print('Validation Loss: %f' % valid_loss.eval())\n",
    "  print('Test Loss: %f' %  test_loss.eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
