{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This turtorials can be found in the link below\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://citrineinformatics.squarespace.com/blog/2015/3/16/machine-learning-for-the-materials-scientist-feature-engineering-model-building\n",
    "\n",
    "https://contact.citrine.io/blog/2015/3/3/machine-learning-mat-sci-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In my previous post, I talked about getting materials data ready for training machine learning-based models. Here, I will take the next step and actually use the dataset we built previously to create predictive models.\n",
    "### Feature Engineering\n",
    "Let’s examine the top few band gaps in our dataset:\n",
    "LiH,2.981\n",
    "BeH2,5.326\n",
    "B9H11,2.9118\n",
    "B2H5,6.3448\n",
    "BH3,5.3234\n",
    "B5H7,3.5551\n",
    "H34C19,5.4526\n",
    "H3N,4.3287\n",
    "H2O,5.5175\n",
    "HF,6.7187\n",
    "Our goal now is to construct a set of input features that a machine learning model could use to predict these band gaps. The features should contain patterns that a machine learning algorithm could identify, if provided with several thousand training examples of how changes in those features influence band gap. For example, what is it about the alkali hydride LiH that causes its band gap to be about 3 eV, whereas the alkaline earth hydride BeH2 has a much wider gap of over 5 eV? Feature engineering is precisely where our expertise and intuition as materials scientists enters the modeling process.\n",
    "These features must be characteristics we either know or can compute for any material whose band gap we wish to predict. This statement implies some important constraints: For example, we might wish to use another property such as formation energy to predict band gap, but we only know formation energies for a relatively small number of compounds. Likewise, crystal structure is extraordinarily important in governing materials behavior, but if we want to use crystal structure features in our model, we can only model materials whose crystal structures are known a priori. We could imagine using machine learning or another approach to first predict crystal structure, and then use the predicted crystal structure as input for a band gap model. Here, rather than daisy-chaining models, we will keep things simple and attempt to simply map a chemical formula (e.g., NaCl) directly to a value for band gap.\n",
    "Let us begin with an extremely simple and naive representation of a chemical compound for our band gap model, which we will later improve upon by injecting some physical insight. We will define a material using a vector wherein the nth component of the vector represents the atomic fraction of the element having atomic number n+1 in the material. Thus, pure H would be:\n",
    "\n",
    "[1, 0, 0, 0, …]\n",
    "\n",
    "Beryllium hydride (BeH2) would be:\n",
    "\n",
    "[0.67, 0, 0, 0.33, 0, 0, …]\n",
    "\n",
    "This representation has the advantage of simplicity, but the disadvantage that it is a rather “bloated” way to express a chemical compound. Clearly, for most materials, the vast majority of the components of the composition vector will be 0. However, this approach is good enough to illustrate next steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pymatgen import Composition, Element\n",
    "from numpy import zeros, mean\n",
    "\n",
    "# Training file containing band gaps extracted from Materials Project\n",
    "# created in previous blog post and linked here\n",
    "trainFile = open(\"bandgapDFT.csv\",\"r\").readlines()\n",
    "\n",
    "# Input: pymatgen Composition object\n",
    "# Output: length-100 vector representing any chemical formula\n",
    "\n",
    "def naiveVectorize(composition):\n",
    "       vector = zeros((MAX_Z))\n",
    "       for element in composition:\n",
    "               fraction = composition.get_atomic_fraction(element)\n",
    "               vector[element.Z - 1] = fraction\n",
    "       return(vector)\n",
    "\n",
    "# Extract materials and band gaps into lists, and construct naive feature set\n",
    "materials = []\n",
    "bandgaps = []\n",
    "naiveFeatures = []\n",
    "\n",
    "MAX_Z = 100 # maximum length of vector to hold naive feature set\n",
    "\n",
    "for line in trainFile:\n",
    "       split = str.split(line, ',')\n",
    "       material = Composition(split[0])\n",
    "       materials.append(material) #store chemical formulas\n",
    "       naiveFeatures.append(naiveVectorize(material)) #create features from chemical formula\n",
    "       bandgaps.append(float(split[1])) #store numerical values of band gaps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Building\n",
    "We now have a (naive) way of converting each material in our training set into a vector for our machine learning model to crunch, and equivalently we can express any new chemical formula with this representation for the purposes of making predictions. Time for the fun part!\n",
    "You’re probably salivating with anticipation as to which amazing machine learning algorithm we are going to use to model band gap. I have some anti-climactic news: Unless you work at Google and are training machine vision systems to autonomously recognize cats in millions of images, don’t worry too much about using the latest and greatest algorithms. While you may be on the cutting edge of materials informatics, you are likely not on the cutting edge of machine learning (nor should you be; you’re a materials scientist!).\n",
    "Before we construct any real models, let us establish a baseline for accuracy with a trivial approach: simply guessing the mean of the band gap distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The MAE of always guessing the average band gap is: 0.728 eV\n"
     ]
    }
   ],
   "source": [
    "# Establish baseline accuracy by \"guessing the average\" of the band gap set\n",
    "# A good model should never do worse.\n",
    "baselineError = mean(abs(mean(bandgaps) - bandgaps))\n",
    "print(\"The MAE of always guessing the average band gap is: \" + str(round(baselineError, 3)) + \" eV\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A sophisticated model should absolutely never do worse than this, or something is very, very wrong! With that in mind, we will start our real modeling effort with a straightforward approach; after all, in the words of Einstein, everything should be made as simple as possible, but not simpler. Let’s begin by attacking our data with a linear ridge regression model. Yes, you read that correctly: We are starting off with a linear model. Before we throw the kitchen sink at the problem and turn to more complex models, we should evaluate the efficacy of a basic approach. Indeed, linear techniques such as logistic regression can--despite their simplicity--perform strikingly well in comparison to more sophisticated algorithms, and offer the advantages of speed and interpretability.\n",
    "\n",
    "Let’s see how a linear ridge regressor does in modeling band gaps using our naive feature set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The MAE of the linear ridge regression band gap model using the naive feature set is: 0.47 eV\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:90: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:90: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:90: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:90: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:90: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:90: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:90: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:90: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:90: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:90: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    }
   ],
   "source": [
    "# Train linear ridge regression model using naive feature set\n",
    "from sklearn import linear_model, cross_validation, metrics, ensemble\n",
    "\n",
    "#alpha is a tuning parameter affecting how regression deals with collinear inputs\n",
    "linear = linear_model.Ridge(alpha = 0.5)  \n",
    "\n",
    "cv = cross_validation.ShuffleSplit(len(bandgaps),n_iter=10, test_size=0.1, random_state=0)\n",
    "\n",
    "scores = cross_validation.cross_val_score(linear, naiveFeatures,bandgaps, cv=cv, scoring='mean_absolute_error')\n",
    "\n",
    "print(\"The MAE of the linear ridge regression band gap model using the naive feature set is: \"+ str(round(abs(mean(scores)), 3)) + \" eV\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s also take a look at the fitted regression coefficients (just a few of the 100 total are shown here):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below are the fitted linear ridge regression coefficients for each feature (i.e., element) in our naive feature set\n",
      "element: coefficient\n",
      "H: 1.87000918111\n",
      "He: 0.0\n",
      "Li: 0.471679018799\n",
      "Be: 0.243871876828\n",
      "B: 0.22915696116\n",
      "C: 0.35449422283\n",
      "N: 1.69078868432\n",
      "O: 2.28865291048\n",
      "F: 3.95035733949\n",
      "Ne: 0.0\n",
      "Na: 0.794273044946\n",
      "Mg: 0.0239525184542\n",
      "Al: -0.00267048056218\n",
      "Si: 0.282300385515\n",
      "P: 0.618296089354\n",
      "S: 1.24048466049\n",
      "Cl: 2.81918301294\n",
      "Ar: 0.0\n",
      "K: 1.09749347606\n",
      "Ca: 0.12033468392\n",
      "Sc: -0.409236168468\n",
      "Ti: -0.675957059589\n",
      "V: -0.704965141051\n",
      "Cr: -0.771816472891\n",
      "Mn: -0.514374807699\n",
      "Fe: -0.431688681554\n",
      "Co: -0.374220544877\n",
      "Ni: -0.289724992867\n",
      "Cu: -0.386990847414\n",
      "Zn: -0.134767025101\n",
      "Ga: -0.0725382805727\n",
      "Ge: -0.00513072046578\n",
      "As: 0.349421293638\n",
      "Se: 0.643767050081\n",
      "Br: 2.38142382545\n",
      "Kr: -0.0871269683031\n",
      "Rb: 0.787731518215\n",
      "Sr: 0.155215849743\n",
      "Y: -0.265178169559\n",
      "Zr: -0.53793665989\n",
      "Nb: -0.933515300424\n",
      "Mo: -0.650034466386\n",
      "Tc: -0.482894971266\n",
      "Ru: -0.310915841206\n",
      "Rh: -0.318854065436\n",
      "Pd: -0.310607753782\n",
      "Ag: -0.514168117419\n",
      "Cd: -0.213668180038\n",
      "In: -0.190448308408\n",
      "Sn: -0.176635763929\n",
      "Sb: -0.0899143360915\n",
      "Te: 0.238347839337\n",
      "I: 2.00809063521\n",
      "Xe: -0.671963409249\n",
      "Cs: 1.00736318797\n",
      "Ba: 0.143666703275\n",
      "La: -0.340574509366\n",
      "Ce: -0.844700152971\n",
      "Pr: -0.344223084535\n",
      "Nd: -0.281635230134\n",
      "Pm: 0.0702321308612\n",
      "Sm: -0.287261521524\n",
      "Eu: -0.833880431748\n",
      "Gd: -0.602233290377\n",
      "Tb: -0.280812407194\n",
      "Dy: -0.326800353497\n",
      "Ho: -0.240896189605\n",
      "Er: -0.212874486417\n",
      "Tm: -0.13554243352\n",
      "Yb: 0.200013260172\n",
      "Lu: -0.176244656746\n",
      "Hf: -0.430315134845\n",
      "Ta: -0.705349830295\n",
      "W: -0.651878342926\n",
      "Re: -0.342127919289\n",
      "Os: -0.349523954803\n",
      "Ir: -0.196730776259\n",
      "Pt: -0.279382478721\n",
      "Au: -0.318049310557\n",
      "Hg: -0.333369589935\n",
      "Tl: -0.247587358459\n",
      "Pb: -0.259123854759\n",
      "Bi: -0.310843338783\n",
      "Po: 0.0\n",
      "At: 0.0\n",
      "Rn: 0.0\n",
      "Fr: 0.0\n",
      "Ra: 0.0\n",
      "Ac: 0.157557825911\n",
      "Th: -0.349686721319\n",
      "Pa: -1.04513381478\n",
      "U: -1.70796093009\n",
      "Np: -2.02837868606\n",
      "Pu: -1.24709486258\n",
      "Am: 0.0\n",
      "Cm: 0.0\n",
      "Bk: 0.0\n",
      "Cf: 0.0\n",
      "Es: 0.0\n",
      "Fm: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Let's see which features are most important for the linear model\n",
    "\n",
    "print(\"Below are the fitted linear ridge regression coefficients for each feature (i.e., element) in our naive feature set\")\n",
    "\n",
    "linear.fit(naiveFeatures, bandgaps) # fit to the whole data set; we're not doing CV here\n",
    "\n",
    "print(\"element: coefficient\")\n",
    "\n",
    "for i in range(MAX_Z):\n",
    "       element = Element.from_Z(i + 1)\n",
    "       print(element.symbol + ': ' + str(linear.coef_[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examining these coefficients reveals an intuitively satisfying result: electronegative elements such as O, Cl, and F tend to strongly increase compounds’ band gaps, whereas metallic elements such as V and Cr are generally associated with smaller band gaps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering Revisited\n",
    "We were honest with ourselves by calling our first approach to feature engineering naive; we can probably do better than an unwieldy 100-component vector for representing materials’ compositions. Let us now construct an alternative feature set, which builds some basic chemical concepts into the resulting model. This feature set will be far more compact than our 100-component composition vector. In particular, we will:\n",
    "1. Order the two elements in the binary compound according to their atomic fraction abundance in the compound\n",
    "2. Express the stoichiometry of a binary compound simply by the ratio of the more abundant element to the less abundant element\n",
    "3. Calculate the difference in electronegativity between the two elements in the binary\n",
    "4. Include the periodic table group numbers of each element in the binary\n",
    "\n",
    "Within this feature space, BeH2 now becomes [ratio, electronegativity_difference, more_abundant_element_group, less_abundant_element_group]:\n",
    "[2.0, 0.63, 1.0, 2.0]\n",
    "Here is the code to create these features for each material in our training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create alternative feature set that is more physically-motivated\n",
    "\n",
    "physicalFeatures = []\n",
    "\n",
    "for material in materials:\n",
    "       theseFeatures = []\n",
    "       fraction = []\n",
    "       atomicNo = []\n",
    "       eneg = []\n",
    "       group = []\n",
    "\n",
    "       for element in material:\n",
    "               fraction.append(material.get_atomic_fraction(element))\n",
    "               atomicNo.append(float(element.Z))\n",
    "               eneg.append(element.X)\n",
    "               group.append(float(element.group))\n",
    "\n",
    "       # We want to sort this feature set\n",
    "       # according to which element in the binary compound is more abundant\n",
    "       mustReverse = False\n",
    "\n",
    "       if fraction[1] > fraction[0]:\n",
    "               mustReverse = True\n",
    "\n",
    "       for features in [fraction, atomicNo, eneg, group]:\n",
    "               if mustReverse:\n",
    "                       features.reverse()\n",
    "       theseFeatures.append(fraction[0] / fraction[1])\n",
    "       theseFeatures.append(eneg[0] - eneg[1])\n",
    "       theseFeatures.append(group[0])\n",
    "       theseFeatures.append(group[1])\n",
    "       physicalFeatures.append(theseFeatures)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have thus “compressed” our previous representation by a factor of 25. Let’s see how well this new feature set works with our same linear ridge regression from before:\n",
    "The MAE of the linear ridge regression band gap model using the naive feature set is: 0.47 eV\n",
    "\n",
    "The MAE of the linear ridge regression band gap model using the physical feature set is: 0.664 eV\n",
    "Our cross-validation MAE actually increased! How can this be? Well, we have changed from a linear regression with 100 coefficients to a linear regression with only four coefficients. It appears that five degrees of freedom (four coefficients plus a constant) are too few to reasonably model band gaps with a linear model. This outcome invites the obvious next step..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Going Beyond a Linear Model: Black-Box Machine Learning\n",
    "\n",
    "As of yet, it is not clear whether our clever new feature set is actually an improvement over the naive composition vector. Indeed, the cross-validated MAE of our linear ridge regression got worse with the much smaller, physically-motivated feature vector. However, a major change in approach remains untested: switching to a nonlinear model that in fact has no functional form.\n",
    "\n",
    "For the purposes of this exercise, we are going to construct a random forest--an ensemble of decision trees. It turns out that, while a single decision tree is often a poor classifier, a collection of many decision trees trained on different subsets of data can be very powerful for modeling data. Random forests have a number of tuning parameters (as with most machine learning algorithms, unfortunately), but here we highlight only the number of trees in the forest; we will choose 10 for computational expediency. Our code will thus construct 10 independent decision trees and average the band gap predictions from each of them. The code to do so is very simple:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:90: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:90: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:90: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:90: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:90: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:90: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:90: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:90: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:90: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The MAE of the linear ridge regression band gap model using the physical feature set is: 0.265 eV\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/sklearn/metrics/scorer.py:90: DeprecationWarning: Scoring method mean_absolute_error was renamed to neg_mean_absolute_error in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    }
   ],
   "source": [
    "rfr = ensemble.RandomForestRegressor(n_estimators=10) #try 10 trees in the forest\n",
    "\n",
    "cv = cross_validation.ShuffleSplit(len(bandgaps),n_iter=10, test_size=0.1, random_state=0)\n",
    "\n",
    "scores = cross_validation.cross_val_score(rfr, physicalFeatures,bandgaps, cv=cv, scoring='mean_absolute_error')\n",
    "print(\"The MAE of the linear ridge regression band gap model using the physical feature set is: \"+ str(round(abs(mean(scores)), 3)) + \" eV\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that a random forest-based approach outperforms linear ridge regression with both feature sets, and the physically-motivated four-feature vector outperforms the naive 100-component composition vector. The random forest trained on the physical features is actually decent at estimating band gaps of materials!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "At the conclusion of this exercise, we have created a random forest-based model that can predict the band gaps of binary compounds with a cross-validated MAE of under 0.3 eV. Not too shabby! From here, the two main levers we have to further enhance our model are: (1) adding more training data (read: email Materials Project and tell them to do more DFT calculations!) and (2) further feature engineering. Probably our simple four-feature approach is not yet optimized. Again, this is where your background as a materials scientist is essential!\n",
    "\n",
    "We should also put our results here in context. Machine learning is indeed a potent tool for analyzing materials data; when used properly, it can produce powerful, original insights. But it is just that: another tool in the materials scientist’s toolbox. Machine learning (or any other computational technique) is not a substitute for scientific judgment or common sense. As with all computational models, garbage in equals garbage out. However, with a combination of robust training data and insightful features, you can build very fast and very effective models of materials behavior--without a supercomputer, and without a computer science degree.\n",
    "\n",
    "Have a new machine learning or statistics-based model of materials? Developed a great feature or descriptor for modeling a particular property? Found a useful materials data set for model-building purposes? Let us know—we’d love to host it on our platform, and reference it back to you!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
